{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CharToIndex import CharToIndex\n",
    "from MyDatasets import Cross_Validation\n",
    "from MyCustomLayer import WeightedTenHotEncodeLayer\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_file_path = r\"data\\tegaki_katsuji\\all_chars_3812.npy\"\n",
    "tokens = CharToIndex(chars_file_path)\n",
    "data_file_path = r\"data\\tegaki_katsuji\\tegaki_distance.npz\"\n",
    "data = np.load(data_file_path,allow_pickle=True)\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(tokens)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Distanced_TenHot_Dataset_sest5(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,chars_file_path,device=torch.device('cpu')):\n",
    "        self.data = data\n",
    "        self.val_idx = []\n",
    "        self.ans_idx = []\n",
    "        self.char2index = CharToIndex(chars_file_path)\n",
    "        self.length = len(data['answer'])-4\n",
    "        self.device = device\n",
    "\n",
    "        values = data['value']\n",
    "        for chars in values:\n",
    "            indexes = []\n",
    "            for idx in map(self.char2index.get_index,chars):\n",
    "                indexes.append(idx)\n",
    "            self.val_idx.append(indexes)\n",
    "\n",
    "        answers = data['answer']\n",
    "        for idx in map(self.char2index.get_index,answers):\n",
    "            self.ans_idx.append(idx)\n",
    "\n",
    "\n",
    "        #距離値付きのten_hot_encodeにvalueを変換\n",
    "        distances = data['distance']\n",
    "        self.distanced_ten_hot_encoded_value = np.zeros(shape=(values.shape[0],VOCAB_SIZE),dtype=np.float32)\n",
    "        for row,indicies in enumerate(self.val_idx):\n",
    "            for id_distance,id_value in enumerate(indicies):\n",
    "                self.distanced_ten_hot_encoded_value[row][id_value]=distances[row][id_distance]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        assert idx < self.length\n",
    "        out_val = self.distanced_ten_hot_encoded_value[idx:idx+5]\n",
    "        out_ans = self.ans_idx[idx+4]\n",
    "        return torch.tensor(out_val).to(self.device),torch.tensor(out_ans).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe3\\x82\\x91'\n",
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe7\\xb8\\x8a'\n"
     ]
    }
   ],
   "source": [
    "data = data\n",
    "val_idx = []\n",
    "ans_idx = []\n",
    "char2index = CharToIndex(chars_file_path)\n",
    "length = len(data['answer'])-8\n",
    "device = device\n",
    "\n",
    "values = data['value']\n",
    "val_idx = []\n",
    "for chars in values:\n",
    "    indexes = []\n",
    "    for idx in map(char2index.get_index,chars):\n",
    "        indexes.append(idx)\n",
    "    val_idx.append(indexes)\n",
    "\n",
    "answers = data['answer']\n",
    "for idx in map(char2index.get_index,answers):\n",
    "    ans_idx.append(idx)\n",
    "\n",
    "distances = data['distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanced_ten_hot_encoded_value = np.zeros(shape=(values.shape[0],VOCAB_SIZE),dtype=np.float32)\n",
    "\n",
    "for row,indicies in enumerate(val_idx):\n",
    "    for id_distance,id_value in enumerate(indicies):\n",
    "        if id_distance == 10:\n",
    "            print(id_value)\n",
    "        distanced_ten_hot_encoded_value[row][id_value]=distances[row][id_distance]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "広法伝宏玄庄去右芸店\t 広\n",
      "゛４いぃＨリＭＸＹ糺\t い\n",
      "分今令劣冷合台会Ｇ什\t 分\n",
      "野時貯第路努好踏畔男\t 野\n",
      "へヘぺべベ八人入久バ\t へ\n",
      "適通過週遍遇道遮逼糸\t 適\n",
      "用風円肉周丹舟月痢何\t 用\n",
      "さ土をざェエき士ょよ\t さ\n",
      "れ札礼花丸托ね孔社九\t れ\n",
      "てマ々で２スセユュえ\t て\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = int(random.random()*len(tokens))\n",
    "\n",
    "hotted_indices = np.argsort(-distanced_ten_hot_encoded_value[i])[:10]\n",
    "for item in range(i,i+10):\n",
    "    hotted_indices = np.argsort(-distanced_ten_hot_encoded_value[item])[:10]\n",
    "    for idx in hotted_indices:\n",
    "        print(tokens.get_decoded_char(idx), end='')\n",
    "    print('\\t',tokens.get_decoded_char(ans_idx[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def show_ans_pred(answers,predictions):\n",
    "    for ans,pred in zip(answers,predictions):\n",
    "        correct = '✓' if ans.item() == pred.item() else '✗'\n",
    "        print(f'{tokens.get_decoded_char(ans.item())}{tokens.get_decoded_char(pred.item()):2} {correct}',end=' ')\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def train(model,train_dataloader,learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = next(iter(train_dataloader))[0].size(0)\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for i,(x,y) in enumerate(train_dataloader):\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y) #損失計算\n",
    "        prediction = output.data.max(1)[1] #予測結果\n",
    "        accuracy += prediction.eq(y.data).sum().item()/batch_size\n",
    "        optimizer.zero_grad() #勾配初期化\n",
    "        loss.backward(retain_graph=True) #逆伝播\n",
    "        optimizer.step()  #重み更新\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    loss_result = running_loss/len(train_dataloader)\n",
    "    accuracy_result = accuracy/len(train_dataloader)\n",
    "\n",
    "    return loss_result,accuracy_result\n",
    "\n",
    "\n",
    "def eval(model,valid_dataloader,is_show_ans_pred=False):\n",
    "    accuracy = 0\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    model.eval()\n",
    "    for x,y in valid_dataloader:\n",
    "        output = model(x)\n",
    "        prediction = output.data.max(1)[1] #予測結果\n",
    "        accuracy += prediction.eq(y.data).sum().item()/batch_size\n",
    "        if is_show_ans_pred:\n",
    "            ans_pred_list=show_ans_pred(y,prediction)\n",
    "            print(ans_pred_list)\n",
    "\n",
    "    return accuracy/len(valid_dataloader)\n",
    "\n",
    "\n",
    "class Proofreader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size,n_layers):\n",
    "        super(Proofreader, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers  = n_layers\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.rnn = nn.RNN(output_size, self.hidden_dim, batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim*2, output_size)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size).to(self.device)\n",
    "\n",
    "        out, hidden = self.rnn(x.float(), hidden)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def get_correct_char(model,valid_dataloader,correct_char):\n",
    "    accuracy = 0\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    model.eval()\n",
    "    for x,y in valid_dataloader:\n",
    "        output = model(x)\n",
    "        prediction = output.data.max(1)[1] #予測結果\n",
    "        accuracy += prediction.eq(y.data).sum().item()/batch_size\n",
    "\n",
    "        for correct,idx in zip(prediction.eq(y.data),y.data):\n",
    "            if correct:\n",
    "                correct_char[idx]+=1\n",
    "\n",
    "\n",
    "    return accuracy/len(valid_dataloader),correct_char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe3\\x82\\x91'\n",
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe7\\xb8\\x8a'\n",
      "Cross Validation: k=[1/1]\n",
      "[          ] 0.0%final_loss: 2.934099,   final_accuracy:0.671875\n",
      "\n",
      "\n",
      "=================================================\n",
      "accuracies: [0.671875]\n",
      "losses: [2.93409939456325]\n",
      "accu average: 0.671875\n",
      "loss average: 2.93409939456325\n"
     ]
    }
   ],
   "source": [
    "from DistancedDatasets import Distanced_TenHot_Dataset_sest5 as MyDataset\n",
    "tegaki_dataset = MyDataset(data,chars_file_path,device=device)\n",
    "\n",
    "final_accuracies = []\n",
    "final_losses = []\n",
    "correct_char=torch.zeros(len(tokens),dtype=torch.int)\n",
    "\n",
    "cross_validation = Cross_Validation(tegaki_dataset)\n",
    "k_num = cross_validation.k_num #デフォルトは10\n",
    "k_num = 1\n",
    "\n",
    "\n",
    "##学習\n",
    "for i in range(k_num):\n",
    "    train_dataset,valid_dataset = cross_validation.get_datasets(k_idx=i)\n",
    "\n",
    "    print(f'Cross Validation: k=[{i+1}/{k_num}]')\n",
    "\n",
    "    train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True) #訓練データのみシャッフル\n",
    "    valid_dataloader=DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "    model = Proofreader(VOCAB_SIZE, hidden_dim=HIDDEN_SIZE, output_size=VOCAB_SIZE, n_layers=1)\n",
    "    # model.load_state_dict(torch.load(\"data/tegaki_katsuji/pre_trained_model.pth\"))\n",
    "\n",
    "    epochs = 1\n",
    "    acc_record=[]\n",
    "    loss_record=[]\n",
    "    start = time.time() #開始時間の設定\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #進捗表示\n",
    "        i = (epoch-1)%10\n",
    "        pro_bar = ('=' * i) + (' ' * (10 - i))\n",
    "        print('\\r[{0}] {1}%'.format(pro_bar, i / 10 * 100.), end='')\n",
    "\n",
    "\n",
    "        loss,acc = train(model,train_dataloader,learning_rate=0.01)\n",
    "\n",
    "        valid_acc = eval(model,valid_dataloader)\n",
    "        loss_record.append(loss)\n",
    "        acc_record.append(valid_acc)\n",
    "\n",
    "\n",
    "        if epoch%10==0:\n",
    "            print(f'\\repoch:[{epoch:3}/{epochs}] | {timeSince(start)} - loss: {loss:.7},  accuracy: {acc:.7},  valid_acc: {valid_acc:.7}')\n",
    "            start = time.time() #開始時間の設定\n",
    "\n",
    "    acc,correct_char=get_correct_char(model,valid_dataloader,correct_char)\n",
    "\n",
    "\n",
    "    print(f'final_loss: {loss_record[-1]:.7},   final_accuracy:{acc_record[-1]:.7}\\n\\n')\n",
    "\n",
    "    final_accuracies.append(acc_record[-1])\n",
    "    final_losses.append(loss_record[-1])\n",
    "\n",
    "print(f'=================================================')\n",
    "print(f'accuracies: {final_accuracies}')\n",
    "print(f'losses: {final_losses}')\n",
    "\n",
    "print(f'accu average: {np.mean(final_accuracies)}')\n",
    "print(f'loss average: {np.mean(final_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a00e993185d0568e3f97167174fa355b1edc030a10525d38e566a001fe2b711"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('Correction-of-misread-characters': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
