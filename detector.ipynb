{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from CharToIndex import CharToIndex\n",
    "from MyDatasets import Cross_Validation\n",
    "# from MyCustomLayer import TenHotEncodeLayer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,chars_file_path,device=torch.device('cpu')):\n",
    "        self.data = data\n",
    "        self.val_idx = []\n",
    "        self.ans_idx = []\n",
    "        self.char2index = CharToIndex(chars_file_path)\n",
    "        self.len = len(data[0])\n",
    "        self.device = device\n",
    "\n",
    "        values = data[0]\n",
    "        self.val_idx = []\n",
    "        for chars in values:\n",
    "            indexes = []\n",
    "            for idx in map(self.char2index.get_index,chars):\n",
    "                indexes.append(idx)\n",
    "            self.val_idx.append(indexes)\n",
    "\n",
    "        answers = data[1]\n",
    "        for idx in map(self.char2index.get_index,answers):\n",
    "            self.ans_idx.append(idx)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        assert idx < self.len\n",
    "        inp = self.val_idx[idx]\n",
    "        #OCRの第一候補と答えが等しければ１、等しくなければ０\n",
    "        if self.val_idx[idx][0] == self.ans_idx[idx]:\n",
    "            tar = 1\n",
    "        else:\n",
    "            tar = 0\n",
    "        return torch.tensor(inp).to(self.device),torch.tensor(tar).to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_file_path = \"/net/nfs2/export/home/ohno/CR_pytorch/data/tegaki_katsuji/all_chars_3812.npy\"\n",
    "tokens = CharToIndex(chars_file_path)\n",
    "file_path = \"/net/nfs2/export/home/ohno/CR_pytorch/data/tegaki_katsuji/tegaki.npy\"\n",
    "data = np.load(file_path,allow_pickle=True)\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(tokens)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tegaki_dataset = BinaryClassDataset(data,chars_file_path,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TenHotEncodeLayer(nn.Module):\n",
    "    def __init__(self, num_tokens):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self,x):\n",
    "        hot_out = torch.zeros(x.size(0),self.num_tokens)\n",
    "\n",
    "        for N in range(x.size(0)):\n",
    "            for F in x[N]:\n",
    "                hot_out[N,F.long()]=1\n",
    "        return hot_out.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detail(x,y,p):\n",
    "    for _x,_y,_p in zip(x,y,p): #バッチ\n",
    "        #xの表示\n",
    "        for __x in _x:\n",
    "            print(tokens.get_decoded_char(int(__x.item())),end='')\n",
    "        print('\\t\\t(予想、答え)=(',end='')\n",
    "        if _p.item() == 1:\n",
    "            print('〇,',end='')\n",
    "        if _p.item() == 0:\n",
    "            print('Ｘ,',end='')\n",
    "        if _y.item() == 1:\n",
    "            print('〇',end='')\n",
    "        if _y.item() == 0:\n",
    "            print('Ｘ',end='')\n",
    "\n",
    "        if _p.item() == _y.item():\n",
    "            print(')--> 正解')\n",
    "        else:\n",
    "            print(')--> 不正解')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = next(iter(train_dataloader))[0].size(0)\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for i,(x,y) in enumerate(train_dataloader):\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y) #損失計算\n",
    "        prediction = output.data.max(1)[1] #予測結果\n",
    "        accuracy += prediction.eq(y.data).sum().item()/batch_size\n",
    "        optimizer.zero_grad() #勾配初期化\n",
    "        loss.backward(retain_graph=True) #逆伝播\n",
    "        optimizer.step()  #重み更新\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    loss_result = running_loss/len(train_dataloader)\n",
    "    accuracy_result = accuracy/len(train_dataloader)\n",
    "\n",
    "    return loss_result,accuracy_result\n",
    "\n",
    "def eval(model,valid_dataloader,is_show_detail=False):\n",
    "    accuracy = 0\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    confusion_matrix = torch.zeros(2,2)\n",
    "    model.eval()\n",
    "\n",
    "    for x,y in valid_dataloader:\n",
    "        output = model(x)\n",
    "        prediction = output.data.max(1)[1] #予測結果\n",
    "        accuracy += prediction.eq(y.data).sum().item()/batch_size\n",
    "\n",
    "        if is_show_detail:\n",
    "          show_detail(x,y,prediction)\n",
    "\n",
    "        for y_true,y_pred in zip(y,prediction):\n",
    "          confusion_matrix[y_true,y_pred]+=1\n",
    "    return accuracy/len(valid_dataloader), confusion_matrix\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,encode_size):\n",
    "    super(Net, self).__init__()\n",
    "    self.encoder = TenHotEncodeLayer(encode_size)\n",
    "    self.fc1 = nn.Linear(encode_size, 128)\n",
    "    self.fc2 = nn.Linear(128, 2)\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.to(self.device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: k=[1/1]\n",
      "epoch:[ 10/11] | 2m 39s - loss: 0.09596165,  accuracy: 0.9608645,  valid_acc: 0.8430707,  precision: 0.6038961,    recall: 0.3536122\n",
      "[          ] 0.0%final_loss: 0.08697265,   final_accuracy:0.8328804\n",
      "\n",
      "\n",
      "*** accuracies: [0.8328804347826086]\n",
      "*** losses: [0.08697264660236852]\n",
      "*** accu average: 0.8328804347826086\n",
      "*** loss average: 0.08697264660236852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_accuracies = []\n",
    "final_losses = []\n",
    "\n",
    "cross_validation = Cross_Validation(tegaki_dataset)\n",
    "k_num = cross_validation.k_num #デフォルトは10\n",
    "# k_num=1\n",
    "\n",
    "\n",
    "##学習\n",
    "for i in range(k_num):\n",
    "    train_dataset,valid_dataset = cross_validation.get_datasets(k_idx=i)\n",
    "\n",
    "    print(f'Cross Validation: k=[{i+1}/{k_num}]')\n",
    "\n",
    "    train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True) #訓練データのみシャッフル\n",
    "    valid_dataloader=DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "    model = Net(encode_size=len(tokens))\n",
    "\n",
    "    epochs = 100\n",
    "    acc_record=[]\n",
    "    loss_record=[]\n",
    "    start = time.time() #開始時間の設定\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #進捗表示\n",
    "        i = (epoch-1)%10\n",
    "        pro_bar = ('=' * i) + (' ' * (10 - i))\n",
    "        print('\\r[{0}] {1}%'.format(pro_bar, i / 10 * 100.), end='')\n",
    "\n",
    "\n",
    "        loss,acc = train(model,train_dataloader,learning_rate=0.01)\n",
    "\n",
    "        valid_acc,conf_mat = eval(model,valid_dataloader)\n",
    "        loss_record.append(loss)\n",
    "        acc_record.append(valid_acc)\n",
    "\n",
    "        if epoch%10==0:\n",
    "            print(f'\\repoch:[{epoch:3}/{epochs}] | {timeSince(start)} - loss: {loss:.7},  accuracy: {acc:.7},  valid_acc: {valid_acc:.7}',end=',  ')\n",
    "\n",
    "            #recall,precision計算\n",
    "            tp = conf_mat[1,1]\n",
    "            tn = conf_mat[0,0]\n",
    "            fp = conf_mat[0,1]\n",
    "            fn = conf_mat[1,0]\n",
    "            print(f'precision: {tn/(tn+fn):.7},    recall: {tn/(tn+fp):.7}')\n",
    "            start = time.time() #開始時間の設定\n",
    "\n",
    "    #学習結果の表示\n",
    "    # eval(model,valid_dataloader,is_show_detail=True)\n",
    "\n",
    "    print(f'final_loss: {loss_record[-1]:.7},   final_accuracy:{acc_record[-1]:.7}\\n\\n')\n",
    "\n",
    "    final_accuracies.append(acc_record[-1])\n",
    "    final_losses.append(loss_record[-1])\n",
    "\n",
    "print(f'*** accuracies: {final_accuracies}')\n",
    "print(f'*** losses: {final_losses}')\n",
    "\n",
    "print(f'*** accu average: {np.mean(final_accuracies)}')\n",
    "print(f'*** loss average: {np.mean(final_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  69.,  194.],\n",
      "        [  47., 1162.]])\n"
     ]
    }
   ],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3f068fdb30bfe91116931ab9f38f90abd8ac3f51d16a38a70c3d538a36b3166"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('CR_pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
