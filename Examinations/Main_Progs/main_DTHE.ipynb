{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from CharToIndex import CharToIndex\n",
    "from MyDatasets import Cross_Validation\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distanced_TenHot_Dataset_For_Main_set5(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,chars_file_path,device=torch.device('cpu')):\n",
    "        self.data = data\n",
    "        self.val_idx = []\n",
    "        self.ans_idx = []\n",
    "        self.char2index = CharToIndex(chars_file_path)\n",
    "        self.length = len(data['answer'])-4\n",
    "        self.device = device\n",
    "\n",
    "        values = data['value']\n",
    "        for chars in values:\n",
    "            indexes = []\n",
    "            for idx in map(self.char2index.get_index,chars):\n",
    "                indexes.append(idx)\n",
    "            self.val_idx.append(indexes)\n",
    "\n",
    "        answers = data['answer']\n",
    "        for idx in map(self.char2index.get_index,answers):\n",
    "            self.ans_idx.append(idx)\n",
    "\n",
    "\n",
    "        #距離値付きのten_hot_encodeにvalueを変換\n",
    "        distances = data['distance']\n",
    "        self.distanced_ten_hot_encoded_value = np.zeros(shape=(values.shape[0],len(self.char2index)),dtype=np.float32)\n",
    "        for row,indicies in enumerate(self.val_idx):\n",
    "            for id_distance,id_value in enumerate(indicies):\n",
    "                self.distanced_ten_hot_encoded_value[row][id_value]=distances[row][id_distance]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        assert idx < self.length\n",
    "\n",
    "        detec_val = self.distanced_ten_hot_encoded_value[idx+4]\n",
    "        proof_val = self.distanced_ten_hot_encoded_value[idx:idx+5]\n",
    "        proof_ans = self.ans_idx[idx+4]\n",
    "\n",
    "        ocr = self.val_idx[idx+4][0] #OCR第一候補\n",
    "\n",
    "        #OCRの第一候補と答えが等しければ１、等しくなければ０\n",
    "        if ocr == self.ans_idx[idx+4]:\n",
    "            detec_ans = 1\n",
    "        else:\n",
    "            detec_ans = 0\n",
    "\n",
    "        detec_val = torch.tensor(detec_val).to(self.device)\n",
    "        detec_ans = torch.tensor(detec_ans).to(self.device)\n",
    "        proof_val = torch.tensor(proof_val).to(self.device)\n",
    "        proof_ans = torch.tensor(proof_ans).to(self.device)\n",
    "        ocr       = torch.tensor(ocr).to(self.device)\n",
    "\n",
    "        return detec_val,detec_ans,proof_val,proof_ans,ocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe3\\x82\\x91'\n",
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe7\\xb8\\x8a'\n"
     ]
    }
   ],
   "source": [
    "chars_file_path = r\"/net/nfs2/export/home/ohno/CR_pytorch/data/tegaki_katsuji/all_chars_3812.npy\"\n",
    "datas_file_path = r\"/net/nfs2/export/home/ohno/CR_pytorch/data/tegaki_katsuji/tegaki_distance.npz\"\n",
    "\n",
    "tokens = CharToIndex(chars_file_path)\n",
    "\n",
    "data = np.load(datas_file_path,allow_pickle=True)\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(tokens)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tegaki_dataset = Distanced_TenHot_Dataset_For_Main_set5(data,chars_file_path,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(detector,proofreader,train_dataloader,learning_rate=0.001):\n",
    "    d_criterion = nn.CrossEntropyLoss()\n",
    "    p_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    d_optim = optim.Adam(detector.parameters(), lr=learning_rate)\n",
    "    p_optim = optim.Adam(proofreader.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = next(iter(train_dataloader))[0].size(0)\n",
    "    d_running_loss = 0\n",
    "    p_running_loss = 0\n",
    "\n",
    "    d_runnning_accu = 0\n",
    "    p_runnning_accu = 0\n",
    "\n",
    "    detector.train()\n",
    "    proofreader.train()\n",
    "    for i,(detec_x,detec_y,proof_x,proof_y,_) in enumerate(train_dataloader):\n",
    "        #検出器の処理\n",
    "        d_output = detector(detec_x)\n",
    "        d_tmp_loss = d_criterion(d_output, detec_y) #損失計算\n",
    "        d_prediction = d_output.data.max(1)[1] #予測結果\n",
    "        d_runnning_accu += d_prediction.eq(detec_y).sum().item()/batch_size\n",
    "        d_optim.zero_grad() #勾配初期化\n",
    "        d_tmp_loss.backward(retain_graph=True) #逆伝播\n",
    "        d_optim.step()  #重み更新\n",
    "        d_running_loss += d_tmp_loss.item()\n",
    "\n",
    "\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(proof_x)\n",
    "        p_tmp_loss = p_criterion(p_output, proof_y) #損失計算\n",
    "        p_prediction = p_output.data.max(1)[1] #予測結果\n",
    "        p_runnning_accu += p_prediction.eq(proof_y.data).sum().item()/batch_size\n",
    "        p_optim.zero_grad() #勾配初期化\n",
    "        p_tmp_loss.backward(retain_graph=True) #逆伝播\n",
    "        p_optim.step()  #重み更新\n",
    "        p_running_loss += p_tmp_loss.item()\n",
    "\n",
    "    p_loss = p_running_loss/len(train_dataloader)\n",
    "    p_accu = p_runnning_accu/len(train_dataloader)\n",
    "    d_loss = d_running_loss/len(train_dataloader)\n",
    "    d_accu = d_runnning_accu/len(train_dataloader)\n",
    "\n",
    "    return d_loss,d_accu,p_loss,p_accu\n",
    "\n",
    "\n",
    "def eval(detector,proofreader,valid_dataloader):\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "\n",
    "    d_runnning_accu = 0\n",
    "    p_runnning_accu = 0\n",
    "\n",
    "    detector.eval()\n",
    "    proofreader.eval()\n",
    "\n",
    "    for detec_x,detec_y,proof_x,proof_y,_ in valid_dataloader:\n",
    "        #検出器の処理\n",
    "        d_output = detector(detec_x)\n",
    "        d_prediction = d_output.data.max(1)[1] #予測結果\n",
    "        d_runnning_accu += d_prediction.eq(detec_y).sum().item()/batch_size\n",
    "\n",
    "\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(proof_x)\n",
    "        p_prediction = p_output.data.max(1)[1] #予測結果\n",
    "        p_runnning_accu += p_prediction.eq(proof_y.data).sum().item()/batch_size\n",
    "\n",
    "    p_accu = p_runnning_accu/len(valid_dataloader)\n",
    "    d_accu = d_runnning_accu/len(valid_dataloader)\n",
    "\n",
    "    return d_accu,p_accu\n",
    "\n",
    "\n",
    "\n",
    "def examination(detector,proofreader,valid_dataloader,show_out=False):\n",
    "    confusion_matrix = torch.zeros(2,2)\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "\n",
    "    runnning_accu = 0\n",
    "    threshold = torch.full((batch_size,2),0.99).to(device)\n",
    "\n",
    "    detector.eval()\n",
    "    proofreader.eval()\n",
    "\n",
    "\n",
    "    for detec_x,_,proof_x,proof_y,ocr_p1 in valid_dataloader:\n",
    "        #検出器の処理\n",
    "        d_output = detector(detec_x)\n",
    "        d_output = F.softmax(d_output,dim=1)\n",
    "        compare_threshold = (d_output > threshold).long()\n",
    "        flg_ocr = compare_threshold[:,1] #ocrの出力を使用するか\n",
    "        ocr_pred = torch.mul(ocr_p1,flg_ocr)\n",
    "\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(proof_x)\n",
    "        rnn_pred = p_output.data.max(1)[1] #RNNの予測結果\n",
    "        flg_rnn = torch.logical_not(flg_ocr,out=torch.empty(batch_size,dtype=torch.long).to(device))#rnnの出力を使用するか\n",
    "        rnn_pred.mul_(flg_rnn)\n",
    "\n",
    "        prediction = torch.add(ocr_pred,rnn_pred)\n",
    "        runnning_accu += prediction.eq(proof_y.data).sum().item()/batch_size\n",
    "\n",
    "        if show_out:\n",
    "            # print('\\nＯＣＲ: ',end='')\n",
    "            # for idx in ocr_p1.data:\n",
    "            #     print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            # print('\\n検出　: ',end='')\n",
    "            # for idx,ocr_idx in zip(ocr_pred.data,ocr_p1.data):\n",
    "            #     if idx == 0:\n",
    "            #         print('[',tokens.get_decoded_char(ocr_idx),']',end='')\n",
    "            #     else :\n",
    "            #         print(tokens.get_decoded_char(ocr_idx),end='')\n",
    "\n",
    "            print('\\n予測　: ',end='')\n",
    "            for idx in prediction:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            # print('\\n正解　: ',end='')\n",
    "            # for idx in proof_y.data:\n",
    "            #     print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            # print()\n",
    "\n",
    "    accuracy = runnning_accu/len(valid_dataloader)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "  def __init__(self,encode_size):\n",
    "    super(Detector, self).__init__()\n",
    "    self.fc1 = nn.Linear(encode_size, 128)\n",
    "    self.fc2 = nn.Linear(128, 2)\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.to(self.device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Proofreader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size,n_layers):\n",
    "        super(Proofreader, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers  = n_layers\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.rnn = nn.RNN(output_size, self.hidden_dim, batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim*2, output_size)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size).to(self.device)\n",
    "        out, hidden = self.rnn(x.float(), hidden)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_output_file_path = r\"/net/nfs2/export/home/ohno/CR_pytorch/results/Main/main_DTHE_threshold_099.txt\"\n",
    "# text_file = open(result_output_file_path,\"a\") #結果の保存\n",
    "\n",
    "\n",
    "cross_validation = Cross_Validation(tegaki_dataset)\n",
    "k_num = cross_validation.k_num #デフォルトは10\n",
    "k_num=0\n",
    "\n",
    "acc_record = []\n",
    "d_acc_record=[]\n",
    "d_loss_record=[]\n",
    "p_acc_record=[]\n",
    "p_loss_record=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##学習\n",
    "for i in range(k_num):\n",
    "    train_dataset,valid_dataset = cross_validation.get_datasets(k_idx=i)\n",
    "\n",
    "    print(f'Cross Validation: k=[{i+1}/{k_num}]')\n",
    "    # text_file.write(f'Cross Validation: k=[{i+1}/{k_num}]\\n')\n",
    "\n",
    "    train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True) #訓練データのみシャッフル\n",
    "    valid_dataloader=DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "    proofreader = Proofreader(VOCAB_SIZE, hidden_dim=HIDDEN_SIZE, output_size=VOCAB_SIZE, n_layers=1)\n",
    "    detector = Detector(encode_size=len(tokens))\n",
    "\n",
    "    proofreader.load_state_dict(torch.load(\"/net/nfs2/export/home/ohno/CR_pytorch/data/proofreader.pth\"))\n",
    "    detector.load_state_dict(torch.load(\"/net/nfs2/export/home/ohno/CR_pytorch/data/detector.pth\"))\n",
    "\n",
    "    epochs = 100\n",
    "    # epochs = 10\n",
    "\n",
    "    start = time.time() #開始時間の設定\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #進捗表示\n",
    "        print(f'\\r{epoch}', end='')\n",
    "\n",
    "        d_loss,d_accu,p_loss,p_accu = train(detector,proofreader,train_dataloader,learning_rate=0.01)\n",
    "\n",
    "        d_val_accu,p_val_accu = eval(detector,proofreader,valid_dataloader)\n",
    "\n",
    "\n",
    "        # print(f'\\n d_loss:{d_loss:.5}, d_accu:{d_accu:.5}, d_val_accu:{d_val_accu:.5}')\n",
    "        # print(f' p_loss:{p_loss:.5}, p_accu:{p_accu:.5}, p_val_accu:{p_val_accu:.5}')\n",
    "\n",
    "        if epoch%10==0:\n",
    "            print(f'\\r epoch:[{epoch:3}/{epochs}]| {timeSince(start)}')\n",
    "            print(f'  Detector| loss:{d_loss:.5}, accu:{d_accu:.5}, val_accu:{d_val_accu:.5}')\n",
    "            print(f'  Proof   | loss:{p_loss:.5}, accu:{p_accu:.5}, val_accu:{p_val_accu:.5}')\n",
    "            # text_file.write(f'\\r epoch:[{epoch:3}/{epochs}]\\n')\n",
    "            # text_file.write(f'  Detector| loss:{d_loss:.5}, accu:{d_accu:.5}, val_accu:{d_val_accu:.5}\\n')\n",
    "            # text_file.write(f'  Proof   | loss:{p_loss:.5}, accu:{p_accu:.5}, val_accu:{p_val_accu:.5}\\n')\n",
    "            start = time.time() #開始時間の設定\n",
    "\n",
    "    #学習結果の表示\n",
    "    accuracy = examination(detector,proofreader,valid_dataloader,show_out=False)\n",
    "\n",
    "    d_loss_record.append(d_loss)\n",
    "    d_acc_record.append(d_val_accu)\n",
    "    p_loss_record.append(p_loss)\n",
    "    p_acc_record.append(p_val_accu)\n",
    "    acc_record.append(accuracy)\n",
    "    print(f' examin accuracy:{acc_record[-1]:.7}\\n\\n')\n",
    "    # text_file.write(f' examin accuracy:{acc_record[-1]:.7}\\n\\n')\n",
    "\n",
    "\n",
    "print(f'=================================================')\n",
    "print(f'Detector \\nacc: {d_acc_record}')\n",
    "print(f'loss: {d_loss_record}')\n",
    "print(f'acc average: {np.mean(d_acc_record)}')\n",
    "print(f'Proof \\nacc: {p_acc_record}')\n",
    "print(f'loss: {p_loss_record}')\n",
    "print(f'Examin \\nacc: {acc_record}')\n",
    "print(f'accu average: {np.mean(acc_record)}')\n",
    "\n",
    "\n",
    "# text_file.write(f'=================================================\\n\\n')\n",
    "# text_file.write(f'Detector \\nacc: {d_acc_record}\\n')\n",
    "# text_file.write(f'loss: {d_loss_record}\\n')\n",
    "# text_file.write(f'acc average: {np.mean(d_acc_record)}\\n')\n",
    "# text_file.write(f'Proof \\nacc: {p_acc_record}\\n')\n",
    "# text_file.write(f'loss: {p_loss_record}\\n')\n",
    "# text_file.write(f'Examin \\nacc: {acc_record}\\n')\n",
    "# text_file.write(f'accu average: {np.mean(acc_record)}\\n')\n",
    "\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "予測　: （この場合通貨とも呼ぱれる）を発行しています。一方で様々な経済取引は国境を越えて行われます。当然、１っの国の通貨と別の国の通貨を\n",
      "予測　: 交換することが必要になります。これに応える市場が外国為替市場です。未ドルと円、ボンドとユーロなどが交換されます。通貨がたくさんあ\n",
      "予測　: ると、本章０１節で物々交換にっいて指適したのと同じような間題が発生します。円を持っている人がポンドを欲する一方、ポンドを持ってい\n",
      "予測　: る人はユー口を欲しがり、ユー口を特っている人はメキシコペソを欲しがっているなどです。こんなとき、物々交換に代わろ貨幣による交換と\n",
      "予測　: 同ぴように、皆が受け入れる「貸幣の中の貸幣」があると話は楽です。現状では来ドルがその役目を果たしています。信頼感がある米ドルと各\n",
      "予測　: 国通貨の市場はできやすくなります。本書の問頭で指適した金融の第ニの役割について説明していきましょう。例えば今年種もみが余っている\n",
      "予測　: つ家が、足りないつ家に融通し、来年（利子を付けて）返してもらえぱ、多方にプラスが発生します。こめ取引は現在のコメと将来のコメの交\n",
      "予測　: 換であることモわかります。現実には、借り手も貸し手もコメ以外の多様なものに対するニーズがあるので、この取引はお金の貸し倍りとして\n",
      "予測　: 実行されます。人は毎年何らかの所得をりぎ、そのー部を消費し、残りを貯母します（銀行預金でいらと、残高ではなく、その年の残高の増分\n",
      "予測　: ）所得がない人が消費をすると、マイナスの貯苦になります。さらに、この人は住宮を買ったり、事業をしていれば機械を買ったいするでしよ\n",
      "予測　: ら。２れが投資です（休や投資信託ではなく、実物資産を買うこと）。貯つび投資を上回、て（下回、て）いる主体を、貯害（投資）超過主体\n",
      "予測　: といいます。あるいは黒字（赤字）主体ともいいます。発展段階にある若い国（Ｉマージング諸国）には、投資機会がふ人だ人にあります。一\n",
      "予測　: 方で所得水準がまだ低く、貯つは不十分です。矢進国は逆です。後者から前者に資金を貸し付けることができれぱ、り方にプラスが発生します\n",
      "予測　: 。イギりスは１８い１９世紀初めにかけてョ一ロッパ大陸から資全を借り入れて産業革命にあてきした。その後、１９世紀末には逆に海外に資\n",
      "予測　: 金を貸し付ける国に転じます。アメりカも寺初、ョー口ッパから資金を侶り入れて国内の鉄道などを整備しっつ発展しました。第２次大戦後、\n",
      "予測　: 国際的な資金の貸し借りに対する規制もあり、発展途上国には資金が流れませんでしたが、１９８Ｏ年代から資全が流れ、１９９０年代以降の\n",
      "予測　: 途上国の経済成長の大きな要因となりました。より結期的に、地震などのー害に見舞われてー時的に生産活動がストップし、所得が大きく低下\n",
      "予測　: した地域や国を考えてみましよう。利子率は黒字主体の貯苦超過を、赤字主体の投資超過とマッチさせるょうに決まります。ですので、貯苦と\n",
      "予測　: 投資の決定要因を考えれば、利子率の決まソ方もわかります。コメの貸し借りの例に戻りましょう。つ業技術が進歩すると、種もみからたくさ\n",
      "予測　: んのっ積が得られます。（リ字）最家は、よりたぐさんコメを借りよらとします。つまり、生産性（あるいはー般的な企業と考えれば期待され\n",
      "予測　: る利間率）が高いほど投資が増えます。一方で、コメがたくさ人あるつ家は、貸し出さずに今日食べてしまってもいいわけです。貨し出すとい\n",
      "予測　: うことは今日食べる量を減らして１年後にたくさ人食べょうということです。将来の消費ょりも今日の消費をどれくらい好むかという程度を、\n",
      "予測　: 難しい言葉ですが、時間選好率といいます。これが高いほど預書は減ります。結弓、生産性（期待利間率）が高いほど、または時間選好幸が言"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9381793478260869"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset,valid_dataset = cross_validation.get_datasets(k_idx=9)\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True) #訓練データのみシャッフル\n",
    "valid_dataloader=DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "proofreader = Proofreader(VOCAB_SIZE, hidden_dim=HIDDEN_SIZE, output_size=VOCAB_SIZE, n_layers=1)\n",
    "detector = Detector(encode_size=len(tokens))\n",
    "\n",
    "proofreader.load_state_dict(torch.load(\"/net/nfs2/export/home/ohno/CR_pytorch/data/proofreader.pth\"))\n",
    "detector.load_state_dict(torch.load(\"/net/nfs2/export/home/ohno/CR_pytorch/data/detector.pth\"))\n",
    "\n",
    "\n",
    "examination(detector,proofreader,valid_dataloader,show_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3f068fdb30bfe91116931ab9f38f90abd8ac3f51d16a38a70c3d538a36b3166"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('CR_pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
