{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe3\\x82\\x91'\n",
      "\u001b[31mERROR: No such char --> \u001b[0mb'\\xe7\\xb8\\x8a'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from CharToIndex import CharToIndex\n",
    "from MyDatasets import Cross_Validation\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#5文字の中心を予測\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,chars_file_path,device=torch.device('cpu')):\n",
    "        self.data = data\n",
    "        self.char2index = CharToIndex(chars_file_path)\n",
    "        self.length = len(data['answer'])-4\n",
    "        self.p_val_idx = torch.zeros((self.length+4,10),dtype=torch.long)\n",
    "        self.p_ans_idx = torch.zeros(self.length+4,dtype=torch.long)\n",
    "        self.d_ans     = torch.zeros(self.length+4,dtype=torch.long)\n",
    "        self.device = device\n",
    "\n",
    "        for i_r,chars in enumerate(data['value']):\n",
    "            for i_c, idx in enumerate(map(self.char2index.get_index,chars)):\n",
    "                self.p_val_idx[i_r][i_c] = idx\n",
    "\n",
    "        for i,char in enumerate(data['answer']):\n",
    "            self.p_ans_idx[i] = self.char2index.get_index(char)\n",
    "            self.d_ans[i] = 1 if self.p_val_idx[i][0] == self.p_ans_idx[i] else 0 #検出器用、OCR第一出力と答えが等しければ１、異なれば０\n",
    "\n",
    "\n",
    "        #距離値付きのten_hot_encodeにvalueを変換\n",
    "        distances = np.nan_to_num(data['distance'])\n",
    "        self.distanced_ten_hot_encoded_value = torch.full((self.length+6,len(self.char2index)),0,dtype=torch.float)\n",
    "        for row,indicies in enumerate(self.p_val_idx):\n",
    "            for id_distance,id_value in enumerate(indicies):\n",
    "                self.distanced_ten_hot_encoded_value[row][id_value]=distances[row][id_distance]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        p_inp = self.p_val_idx[index:index+5,0].to(device)\n",
    "        p_tar = self.p_ans_idx[index+2].to(device)\n",
    "        d_ans = self.d_ans[index+2].to(device)\n",
    "        distance = self.distanced_ten_hot_encoded_value[index+2].to(device)\n",
    "        return distance,d_ans,p_inp,p_tar\n",
    "\n",
    "\n",
    "\n",
    "class Proofreader(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size,n_layers):\n",
    "        super(Proofreader, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers  = n_layers\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size,embedding_dim=256)\n",
    "        self.rnn = nn.RNN(256, self.hidden_dim, batch_first=True,bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(self.hidden_dim*2, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "    def forward(self, x, distance):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x.long())\n",
    "        hidden = self.init_hidden(batch_size).to(self.device)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out[:,2,:]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        out.mul_(distance)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Detector(nn.Module):\n",
    "  def __init__(self,encode_size):\n",
    "    super(Detector, self).__init__()\n",
    "    self.fc1 = nn.Linear(encode_size, 128)\n",
    "    self.fc2 = nn.Linear(128, 2)\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.to(self.device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "chars_file_path = r\"/net/nfs2/export/home/ohno/CR_pytorch/data/tegaki_katsuji/all_chars_3812.npy\"\n",
    "datas_file_path = r\"/net/nfs2/export/home/ohno/CR_pytorch/data/tegaki_katsuji/tegaki_distance.npz\"\n",
    "tokens = CharToIndex(chars_file_path)\n",
    "data = np.load(datas_file_path,allow_pickle=True)\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(tokens)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tegaki_dataset = MyDataset(data,chars_file_path,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval(proofreader,valid_dataloader):\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    p_runnning_accu = 0\n",
    "    proofreader.eval()\n",
    "\n",
    "    for d_x,d_y,p_x,p_y in valid_dataloader:\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(p_x,d_x)\n",
    "        p_prediction = p_output.data.max(1)[1] #予測結果\n",
    "        p_runnning_accu += p_prediction.eq(p_y.data).sum().item()/batch_size\n",
    "\n",
    "    p_accu = p_runnning_accu/len(valid_dataloader)\n",
    "\n",
    "    return p_accu\n",
    "\n",
    "def examination(detector,proofreader,valid_dataloader,show_out=False):\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    threshold = torch.full((batch_size,2),1).to(device)\n",
    "    runnning_accu = 0\n",
    "    rnn_collect_cnt=0\n",
    "    ocr_collect_cnt=0\n",
    "    detector.eval()\n",
    "    proofreader.eval()\n",
    "    for d_x,d_y,p_x,p_y in valid_dataloader:\n",
    "        #検出器の処理\n",
    "        ocr_pred = p_x[:,2] #OCR第一候補\n",
    "        d_output = detector(d_x)\n",
    "        compare = (F.softmax(d_output,dim=1) >= threshold).long()\n",
    "        flg_ocr = compare.data.max(1)[1]\n",
    "        ocr = torch.mul(ocr_pred,flg_ocr)\n",
    "\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(p_x,d_x)\n",
    "        rnn_pred = p_output.data.max(1)[1] #RNNの予測結果\n",
    "        flg_rnn = torch.logical_not(flg_ocr,out=torch.empty(batch_size,dtype=torch.long).to(device))#rnnの出力を使用するか\n",
    "        rnn = torch.mul(rnn_pred,flg_rnn)\n",
    "\n",
    "        prediction = torch.add(ocr,rnn)\n",
    "        runnning_accu += prediction.eq(p_y.data).sum().item()/batch_size\n",
    "        rnn_collect_cnt += prediction.eq(p_y.data).sum().item()\n",
    "        ocr_collect_cnt += ocr_pred.eq(p_y.data).sum().item()\n",
    "        if show_out:\n",
    "            print('\\nＯＣＲ: ',end='')\n",
    "            for idx in ocr_pred.data:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print('\\nＲＮＮ: ',end='')\n",
    "            for idx in rnn_pred.data:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print('\\n検出　: ',end='')\n",
    "            for idx,ocr_idx in zip(ocr.data,ocr_pred.data):\n",
    "                if idx == 0:\n",
    "                    print('[',tokens.get_decoded_char(ocr_idx),']',end='')\n",
    "                else :\n",
    "                    print(tokens.get_decoded_char(ocr_idx),end='')\n",
    "\n",
    "            print('\\n予測　: ',end='')\n",
    "            for idx in prediction:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print('\\n正解　: ',end='')\n",
    "            for idx in p_y.data:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print()\n",
    "    accuracy = runnning_accu/len(valid_dataloader)\n",
    "    return accuracy,rnn_collect_cnt,ocr_collect_cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def examination2(proofreader,valid_dataloader,show_out=False):\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    threshold = torch.full((batch_size,2),1).to(device)\n",
    "    runnning_accu = 0\n",
    "    rnn_collect_cnt=0\n",
    "    ocr_collect_cnt=0\n",
    "\n",
    "    proofreader.eval()\n",
    "    for d_x,d_y,p_x,p_y in valid_dataloader:\n",
    "        ocr_pred = p_x[:,2] #OCR第一候補\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(p_x,d_x)\n",
    "        rnn_pred = p_output.data.max(1)[1] #RNNの予測結果\n",
    "        runnning_accu += rnn_pred.eq(p_y.data).sum().item()/batch_size\n",
    "\n",
    "        if show_out:\n",
    "            print('\\nＯＣＲ: ',end='')\n",
    "            for idx in ocr_pred.data:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print('\\nＲＮＮ: ',end='')\n",
    "            for idx in rnn_pred.data:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print('\\n正解　: ',end='')\n",
    "            for idx in p_y.data:\n",
    "                print(tokens.get_decoded_char(idx),end='')\n",
    "\n",
    "            print()\n",
    "    accuracy = runnning_accu/len(valid_dataloader)\n",
    "    return accuracy,rnn_collect_cnt,ocr_collect_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def examination3(proofreader,valid_dataloader):\n",
    "    batch_size = next(iter(valid_dataloader))[0].size(0)\n",
    "    count_arr = np.zeros((len(tokens),2),dtype=int)\n",
    "    success_cnt=0\n",
    "    misread_cnt=0\n",
    "    fatal_cnt = 0\n",
    "    proofreader.eval()\n",
    "    for d_x,d_y,p_x,p_y in valid_dataloader:\n",
    "        ocr_pred = p_x[:,2] #OCR第一候補\n",
    "        #修正器の処理\n",
    "        p_output = proofreader(p_x,d_x)\n",
    "        rnn_pred = p_output.data.max(1)[1] #RNNの予測結果\n",
    "\n",
    "        for ocr,pred,ans  in zip(ocr_pred,rnn_pred,p_y):\n",
    "            if ocr != ans and pred == ans:\n",
    "                count_arr[ans.item()][0]+=1 #ocrの間違いを修正できた回数\n",
    "                success_cnt+=1\n",
    "            if ocr != ans:\n",
    "                count_arr[ans.item()][1]+=1 #ocrの間違い回数\n",
    "                misread_cnt+=1\n",
    "            if ocr == ans and pred != ans:\n",
    "                fatal_cnt+=1\n",
    "\n",
    "    return count_arr,success_cnt,fatal_cnt,misread_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ＯＣＲ: もご刹囲゛ただきまして．誠にありがと５ござぃます。さて１お客様の定韻・足期貯金は１右訪のとおり満期を迎えますので．ご案内ぃたレ末\n",
      "ＲＮＮ: もご到囲いただきまして<UNK>我にありがとらございます。さて１お客様の定誤<UNK>定期貯金は１右誌のとおリ満期を迎えますので<UNK>ご案内いたし末\n",
      "正解　: もご利用いただきまして、誠にありがとうございます。さて、お客様の定額・定期預金は、右記のとおり満期を迎えますので、ご案内いたしま\n",
      "\n",
      "ＯＣＲ: す．今後ともー層のご愛顧を賜りますょうお願ぃ申しあげますっ（払戻しおょび預け替えのお手続きの際に必要となる書類等）貯金証書１総合\n",
      "ＲＮＮ: す<UNK>今後ともー層のご愛顧を賜りますようお験い中しあげますつ（払戻しおよび預け替えのお手続きの際に必要となる書類等）貯金証書１総合\n",
      "正解　: す。今後とも一層のご愛顧を賜りますようお願い申し上げます。（払戻しおよび預け替えのお手続きの際に必要となる書類等）貯金証書、総合\n",
      "\n",
      "ＯＣＲ: 口座通帳（無遍帳型総合ロ座の場合はキャッシュカード１（ぉ届け印おょびご本人であるこヒを碓認できる証明書類（お名前・ご住所・生年月\n",
      "ＲＮＮ: ロ座通帳（無遍限型総合ロ座の場合はキャッシュカードノ（お届け印およびご本んであることを確認できる証明書類（お名前、ご住所・生年月\n",
      "正解　: 口座通帳（無通帳型総合口座の場合はキャッシュカード）、お届け印およびご本人であることを確認できる証明書類（お名前・ご住所・生年月\n",
      "\n",
      "ＯＣＲ: 日の入）た運転免許証等１法人名義っ場合は登認事項証明書等．団体名義の場合は規約の写し筆もああせてお持５くださぃ）このことにつ゛て\n",
      "ＲＮＮ: 日の入）た運転免評証等１法人名義つ場合は登認事漠証明書等<UNK>団体名義の場合は規約の軍し箏もあわせてお持ちください）このことについて\n",
      "正解　: 日の入った運転免許証等、法人名義の場合は登記事項証明書等、団体名義の場合は規約の写し等もあわせてお持ちください）このことについて\n",
      "\n",
      "ＯＣＲ: に日本争術振輿会か５別添のとおソ通知があルまレた。フきまレては司卦の研究者使用ルしルと科研費ハンドブゥ７もごニ送頂き二盈究童旦過\n",
      "ＲＮＮ: に日本争術振興会から別逵のとおリ通知があルました。っきましては周郵の研究者使用ルしルと科研悪ハンドブッ７もご二送頂きニ亜究愛<UNK>過\n",
      "正解　: 、日本学術振興会から別添のとおり通知がありました。つきましては同封の研究者使用ルールと科研費ハンドブックをご一読頂き、研究費の適\n",
      "\n",
      "ＯＣＲ: 正玉使用上フいてア尽ろ貢き王、よ３お願ここ了すＴ科字は階段状に癸展すＺ．革命的な埋論び出るとパラゲイムか大き（転換し（ぃわゆるプ\n",
      "ＲＮＮ: 正玉便用上フいてア尽る貢き天、よ３お顕ここますニ科字は階段状に発展すそ<UNK>革命的な埋論び出るとパラかィムか大きし転換ししいわゆるプ\n",
      "正解　: 正な使用についてご尽力頂きますようお願いします。化学は階段状に発展する。革命的な理論が出るとパラダイムが大きく転換し（いわゆるブ\n",
      "\n",
      "ＯＣＲ: レーケスルーが超こソ）広゛分野へ適用されて゛く。ジャンプしてー段上に昇るのである。しかしその後１どんどん問題が評細になソ、平板な\n",
      "ＲＮＮ: レークスルーが超こり）広い分野へ適用されていく。ジャンプしてー段上に昇るのである。しかしその後１どんどん問題が評細になり、平枝な\n",
      "正解　: レークスルーが起こり）広い分野へ適用されていく。ジャンプして一段上に昇るのである。しかしその後、どんどん問題が詳細になり、平板な\n",
      "\n",
      "ＯＣＲ: 通常科学っ時代を迎える・゛ゎぱ踊り場である。Ｚこに何５かの矛盾や通用不可能な課題があれぱ．そこか５また新たな飛躍ぶ起こるのだ。と\n",
      "ＲＮＮ: 通常科学つ時代を迎える<UNK>いわば踊り場である。そこに何らかの矛盾や通用不可能な課題があれば<UNK>そこからまた新たな飛躍な起こるのだ。と\n",
      "正解　: 通常科学の時代を迎える。いわば踊り場である。そこに何らかの矛盾や適用不可能な課題があれば、そこからまた新たな飛躍が起こるのだ。と\n",
      "\n",
      "ＯＣＲ: ころが１踞リ塀が広゛場合（一見してすべてが解決でをるかのように錯覧する２ともある．そんなとぎに決ま）て言われるの即（科学の終晋よ\n",
      "ＲＮＮ: ころが１距り塀が広い場合（一見してすべてが解決でをるかのように暗員することもある。そんなときに決まって言われるの即（科学の終晋よ\n",
      "正解　: ころが、踊り場が広い場合、一見してすべてが解決できるかのように錯覚することもある。そんなときに決まって言われるのが「科学の終焉」\n",
      "\n",
      "ＯＣＲ: である。科学は行き着くところまで行を着ぃたしもラ何もすることはなぃ１と考えたくなるか５だ。技術聞襲にとぅても科学が重要であること\n",
      "ＲＮＮ: である。科学は行き着くとこるまで行を着いた、もう何もすることはない<UNK>と考えたくなるからだ。技術聞襲にとっても科学が重要であること\n",
      "正解　: である。科学は行き着くところまで行き着いた、もう何もすることはない、と考えたくなるからだ。技術開発にとっても科学が重要であること\n",
      "\n",
      "ＯＣＲ: が認識されるにフれ．技術産莱におぃて意識的に科学を取リ入れる試計がなされるよ３にな、た。すでにー九世紀にぉぃてドイッで最初の産業\n",
      "ＲＮＮ: が認識されるにフれ、技術産業において意識的に科学を取り入れる試計がなされるようにな、た。すでに一兀世紀においてドイッで最初の産業\n",
      "正解　: が認識されるにつれ、技術産業において意識的に科学を取り入れる試みがなされるようになった。すでに一九世紀においてドイツで最初の産業\n",
      "\n",
      "ＯＣＲ: 用化学実験所ぴ設立さえたのに続さ゛くっかの産薬で付属の研究機関が創設された。技術進歩を遂げるためには．応用研究と゛えども基礎研究\n",
      "ＲＮＮ: 用化学実験所び設立されたのに続きいくつかの産薬で付属の研究機関が創設された。技術進歩を遂げるためには<UNK>応用研究といえども基礎研究\n",
      "正解　: 用化学実験所が設立されたのに続きいくつかの産業で付属の研究機関が創設された。技術進歩を遂げるためには、応用研究といえども基礎研究\n",
      "\n",
      "ＯＣＲ: から稜み上げることの重要性ぷ認識されろょらになったためである．そっ典型は．ＡＴ２Ｔ社のべル研究所であリ１工ジソンが設立したモンロ\n",
      "ＲＮＮ: から積み上げることの重要性が認識されるようになったためである。その典型はしＡＴ２<UNK>社のベル研究所であり１エジリンが設立したもンロ\n",
      "正解　: から積み上げることの重要性が認識されるようになったためである。その典型は、ＡＴ＆Ｔ社のベル研究所であり、エジソンが設立したモンロ\n",
      "\n",
      "ＯＣＲ: ーパー７研究所だろう。ぞ２では．技術者だけでなく（数学・物理学・化学など基礎科孕の研究者をも雇用して共同研究と゛づ形で開発研窓を\n",
      "ＲＮＮ: ーパー７研究所だるう。そこでは、技術者だけでなくし数学・物理学・化学など基礎科争の研究者をも雇用して共同研究という形で開発研窓を\n",
      "正解　: ーパーク研究所だろう。そこでは、技術者だけでなく、数学・物理学・化学など基礎科学の研究者をも雇用して共同研究という形で開発研究を\n",
      "\n",
      "ＯＣＲ: 推し進めたのだ．当然、社会全体へヘ科尋の影響カが強くなソ１職業としての科孕者の数が急増することになフた々広げた資料のー番上に載っ\n",
      "ＲＮＮ: 推し進めたのだ。当然、社会全体への科軍の影響カが強くなり<UNK>職業として。科学者の数が急増することになった々広げた資料の一番上に我っ\n",
      "正解　: 推し進めたのだ。当然、社会全体への科学の影響力が強くなり、職業としての科学者の数が急増することになった。広げた資料の一番上に載っ\n",
      "\n",
      "ＯＣＲ: ていたＸモを手に、神谷は山崎に専問的な質問を勺けた。それから小一時間ほどの時間は、栽刊の打ち合あせと゛うよソも、神谷の興味と閑心\n",
      "ＲＮＮ: ていたメもを手に、神昏は山峪に専間的な質間を力けた。それか"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_292743/2487643559.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamination2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproofreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0macc_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mexamination2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproofreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# cnt_arr,suc_cnt,fat_cnt,mis_cnt = examination3(proofreader,valid_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# success_cnt += suc_cnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_292743/1587063966.py\u001b[0m in \u001b[0;36mexamination2\u001b[0;34m(proofreader, valid_dataloader, show_out)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nＲＮＮ: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrnn_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_decoded_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n正解　: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CR_pytorch/class/CharToIndex.py\u001b[0m in \u001b[0;36mget_decoded_char\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[31m\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"ERROR: No such index -->\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\" \\033[0m{index}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CR_pytorch/class/CharToIndex.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[31m\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"ERROR: No such index -->\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\" \\033[0m{index}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CR_pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validation = Cross_Validation(tegaki_dataset)\n",
    "k_num = cross_validation.k_num #デフォルトは10\n",
    "# k_num=1\n",
    "acc_record=[]\n",
    "all_rnn = 0\n",
    "all_ocr = 0\n",
    "count_arr = np.zeros((len(tokens),2),dtype=int)\n",
    "success_cnt=0\n",
    "misread_cnt=0\n",
    "fatal_cnt = 0\n",
    "##学習\n",
    "for k_idx in range(k_num):\n",
    "    train_dataset,valid_dataset = cross_validation.get_datasets(k_idx=k_idx)\n",
    "    valid_dataloader=DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "\n",
    "    proofreader = Proofreader(VOCAB_SIZE, hidden_dim=HIDDEN_SIZE, output_size=VOCAB_SIZE, n_layers=1)\n",
    "    proofreader.load_state_dict(torch.load(\"/net/nfs2/export/home/ohno/CR_pytorch/Culmination/Learned_models/proof_k\"+str(k_idx+1)))\n",
    "    acc,_,_ = examination2(proofreader,valid_dataloader)\n",
    "    acc_record.append(acc)\n",
    "    examination2(proofreader,valid_dataloader,show_out=True)\n",
    "    # cnt_arr,suc_cnt,fat_cnt,mis_cnt = examination3(proofreader,valid_dataloader)\n",
    "    # success_cnt += suc_cnt\n",
    "    # misread_cnt += mis_cnt\n",
    "    # fatal_cnt   += fat_cnt\n",
    "print(np.mean(acc_record))\n",
    "acc_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in np.argsort(-count_arr[:,0]):\n",
    "#     if count_arr[idx][0] == 0 and count_arr[idx][1] >0:\n",
    "#         print(tokens.get_decoded_char(idx),end=' ')\n",
    "#         print(count_arr[idx][0],'回', end=' ')\n",
    "#         print(count_arr[idx][1],'回')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15011"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tegaki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3f068fdb30bfe91116931ab9f38f90abd8ac3f51d16a38a70c3d538a36b3166"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('CR_pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
